{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6MPjfT5NrKQ"
   },
   "source": [
    "<a align=\"left\" href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\n",
    "<img width=\"1024\", src=\"https://user-images.githubusercontent.com/26833433/125273437-35b3fc00-e30d-11eb-9079-46f313325424.png\"></a>\n",
    "\n",
    "This is the **official YOLOv5 ðŸš€ notebook** by **Ultralytics**, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n",
    "For more information please visit https://github.com/ultralytics/yolov5 and https://ultralytics.com. Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Clone repo, install dependencies and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "3809e5a9-dd41-4577-fe62-5531abf7cca2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v6.0-195-gfd55271 torch 1.9.1 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete  (12 CPUs, 15.8 GB RAM, 346.9/475.8 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 1. Inference\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "```shell\n",
    "python detect.py --source 0  # webcam\n",
    "                          img.jpg  # image \n",
    "                          vid.mp4  # video\n",
    "                          path/  # directory\n",
    "                          path/*.jpg  # glob\n",
    "                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR9ZbuQCH7FX",
    "outputId": "8f7e6588-215d-4ebd-93af-88b871e770a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['14_04_16_50.pt'], source=new_test, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  v6.0-195-gfd55271 torch 1.9.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 444 layers, 86220517 parameters, 0 gradients, 204.1 GFLOPs\n",
      "image 1/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_1.png: 640x640 1 pikachu, Done. (3.136s)\n",
      "image 2/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_10.jpg: 384x640 1 pikachu, Done. (1.354s)\n",
      "image 3/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_100.png: 640x640 1 garchomp, Done. (1.797s)\n",
      "image 4/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_101.jpg: 384x640 1 garchomp, Done. (1.111s)\n",
      "image 5/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_102.png: 640x640 1 garchomp, Done. (1.936s)\n",
      "image 6/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_103.jpg: 384x640 1 garchomp, Done. (1.204s)\n",
      "image 7/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_105.png: 608x640 1 garchomp, Done. (1.820s)\n",
      "image 8/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_106.jpg: 384x640 2 garchomps, Done. (1.158s)\n",
      "image 9/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_107.jpg: 384x640 1 garchomp, Done. (1.086s)\n",
      "image 10/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_108.jpg: 384x640 3 garchomps, Done. (1.098s)\n",
      "image 11/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_109.png: 640x640 1 garchomp, Done. (1.729s)\n",
      "image 12/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_11.jpg: 320x640 1 pikachu, Done. (0.909s)\n",
      "image 13/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_111.jpg: 384x640 3 garchomps, 1 lucario, Done. (1.068s)\n",
      "image 14/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_112.jpg: 640x480 1 garchomp, Done. (1.362s)\n",
      "image 15/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_113.jpg: 320x640 1 garchomp, Done. (0.947s)\n",
      "image 16/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_114.png: 384x640 1 garchomp, Done. (1.192s)\n",
      "image 17/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_117.png: 352x640 1 garchomp, Done. (1.223s)\n",
      "image 18/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_118.png: 320x640 1 garchomp, Done. (0.893s)\n",
      "image 19/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_119.jpg: 384x640 1 garchomp, Done. (1.302s)\n",
      "image 20/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_12.png: 480x640 1 pikachu, Done. (1.382s)\n",
      "image 21/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_120.png: 640x640 1 lucario, Done. (1.769s)\n",
      "image 22/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_121.jpg: 640x640 1 lucario, Done. (1.787s)\n",
      "image 23/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_123.jpg: 640x640 1 lucario, Done. (1.828s)\n",
      "image 24/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_125.png: 384x640 1 lucario, Done. (1.088s)\n",
      "image 25/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_126.png: 384x640 1 lucario, Done. (1.114s)\n",
      "image 26/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_127.png: 640x640 1 lucario, Done. (1.809s)\n",
      "image 27/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_128.jpg: 640x480 1 lucario, Done. (1.378s)\n",
      "image 28/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_13.png: 512x640 1 pikachu, Done. (1.425s)\n",
      "image 29/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_130.jpg: 384x640 1 lucario, Done. (1.122s)\n",
      "image 30/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_131.PNG: 384x640 1 lucario, Done. (1.110s)\n",
      "image 31/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_132.jpg: 320x640 1 lucario, Done. (0.898s)\n",
      "image 32/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_133.jpg: 448x640 1 lucario, Done. (1.286s)\n",
      "image 33/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_136.jpg: 640x416 1 lucario, Done. (1.219s)\n",
      "image 34/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_137.jpg: 416x640 1 lucario, Done. (1.214s)\n",
      "image 35/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_138.jpg: 480x640 1 lucario, Done. (1.313s)\n",
      "image 36/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_139.png: 576x640 1 lucario, Done. (1.614s)\n",
      "image 37/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_14.png: 352x640 1 pikachu, Done. (1.013s)\n",
      "image 38/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_142.png: 416x640 1 lucario, Done. (1.175s)\n",
      "image 39/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_143.jpg: 384x640 1 lucario, Done. (1.130s)\n",
      "image 40/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_144.jpeg: 384x640 1 lucario, Done. (1.082s)\n",
      "image 41/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_145.jpg: 640x640 1 lucario, Done. (1.773s)\n",
      "image 42/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_148.jpg: 640x640 1 lucario, Done. (1.723s)\n",
      "image 43/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_149.png: 640x640 1 thundurus, Done. (1.726s)\n",
      "image 44/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_15.png: 640x640 1 pikachu, Done. (1.726s)\n",
      "image 45/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_150.jpg: 384x640 1 thundurus, Done. (1.152s)\n",
      "image 46/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_151.jpg: 384x640 1 thundurus, Done. (1.122s)\n",
      "image 47/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_152.png: 640x640 1 thundurus, Done. (1.789s)\n",
      "image 48/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_154.jpg: 640x480 1 thundurus, Done. (1.348s)\n",
      "image 49/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_155.jpg: 192x640 1 staraptor, 1 thundurus, Done. (0.647s)\n",
      "image 50/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_156.jpg: 384x640 1 thundurus, Done. (1.109s)\n",
      "image 51/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_157.jpg: 640x480 1 thundurus, Done. (1.366s)\n",
      "image 52/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_16.jpg: 384x640 1 pikachu, Done. (1.150s)\n",
      "image 53/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_160.jpg: 640x480 1 thundurus, Done. (1.362s)\n",
      "image 54/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_161.png: 352x640 2 thunduruss, Done. (1.181s)\n",
      "image 55/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_162.png: 608x640 1 thundurus, Done. (2.003s)\n",
      "image 56/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_163.jpg: 640x416 1 thundurus, Done. (2.092s)\n",
      "image 57/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_166.png: 640x640 1 thundurus, Done. (2.132s)\n",
      "image 58/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_167.jpg: 640x480 1 thundurus, Done. (1.573s)\n",
      "image 59/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_17.jpg: 384x640 1 pikachu, Done. (1.248s)\n",
      "image 60/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_18.jpg: 480x640 1 pikachu, Done. (1.528s)\n",
      "image 61/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_2.jpg: 640x640 1 pikachu, Done. (2.006s)\n",
      "image 62/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_22.jpg: 640x640 1 eevee', Done. (1.938s)\n",
      "image 63/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_23.jpg: 640x640 1 eevee', Done. (1.949s)\n",
      "image 64/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_25.jpg: 640x576 1 eevee', Done. (2.031s)\n",
      "image 65/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_27.png: 384x640 1 eevee', Done. (1.507s)\n",
      "image 66/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_28.png: 384x640 1 eevee', Done. (1.795s)\n",
      "image 67/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_29.jpg: 384x640 1 eevee', Done. (1.565s)\n",
      "image 68/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_3.jpg: 640x640 1 pikachu, Done. (2.116s)\n",
      "image 69/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_34.png: 640x608 1 eevee', Done. (2.097s)\n",
      "image 70/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_35.png: 576x640 1 eevee', Done. (1.881s)\n",
      "image 71/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_36.png: 640x640 1 eevee', Done. (1.968s)\n",
      "image 72/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_37.png: 384x640 2 eevee's, Done. (1.315s)\n",
      "image 73/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_44.jpg: 384x640 1 staraptor, Done. (1.240s)\n",
      "image 74/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_45.jpg: 480x640 1 staraptor, Done. (1.415s)\n",
      "image 75/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_46.png: 384x640 1 staraptor, Done. (1.224s)\n",
      "image 76/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_47.jpg: 640x480 1 staraptor, Done. (1.415s)\n",
      "image 77/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_48.png: 512x640 1 staraptor, Done. (1.620s)\n",
      "image 78/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_49.jpg: 640x544 1 staraptor, Done. (2.078s)\n",
      "image 79/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_58.jpg: 640x448 1 staraptor, Done. (1.474s)\n",
      "image 80/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_59.png: 640x576 1 staraptor, Done. (1.769s)\n",
      "image 81/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_60.jpg: 544x640 1 infernape, Done. (1.720s)\n",
      "image 82/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_61.png: 640x640 1 infernape, Done. (1.960s)\n",
      "image 83/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_62.png: 512x640 1 infernape, Done. (1.584s)\n",
      "image 84/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_63.png: 384x640 1 infernape, Done. (1.349s)\n",
      "image 85/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_65.png: 640x640 1 infernape, Done. (2.023s)\n",
      "image 86/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_66.png: 544x640 2 infernapes, Done. (1.599s)\n",
      "image 87/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_67.png: 640x480 1 infernape, Done. (1.505s)\n",
      "image 88/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_68.png: 640x640 1 infernape, Done. (1.976s)\n",
      "image 89/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_70.jpg: 640x576 1 infernape, Done. (1.724s)\n",
      "image 90/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_71.png: 384x640 1 infernape, Done. (1.186s)\n",
      "image 91/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_72.png: 640x640 1 infernape, Done. (1.866s)\n",
      "image 92/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_73.jpg: 480x640 1 infernape, Done. (1.490s)\n",
      "image 93/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_74.png: 384x640 1 infernape, Done. (1.286s)\n",
      "image 94/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_83.png: 640x512 1 staraptor, Done. (1.647s)\n",
      "image 95/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_84.png: 640x640 1 greninja, Done. (1.924s)\n",
      "image 96/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_85.png: 640x640 1 greninja, Done. (1.898s)\n",
      "image 97/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_86.png: 608x640 1 greninja, Done. (1.740s)\n",
      "image 98/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_87.png: 640x480 1 greninja, Done. (1.474s)\n",
      "image 99/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_88.png: 512x640 1 greninja, Done. (1.505s)\n",
      "image 100/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_89.png: 640x640 1 greninja, Done. (1.975s)\n",
      "image 101/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_90.jpg: 640x640 1 greninja, Done. (1.899s)\n",
      "image 102/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_91.jpg: 352x640 1 greninja, Done. (1.137s)\n",
      "image 103/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_92.jpg: 384x640 1 greninja, Done. (1.154s)\n",
      "image 104/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_93.jpg: 384x640 1 greninja, Done. (1.205s)\n",
      "image 105/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_94.jpg: 384x640 3 greninjas, 1 lucario, Done. (1.161s)\n",
      "image 106/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_98.png: 640x640 1 garchomp, Done. (1.868s)\n",
      "image 107/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\train_99.png: 640x640 1 garchomp, Done. (1.815s)\n",
      "image 108/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_100.jpg: 320x640 1 garchomp, Done. (0.963s)\n",
      "image 109/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_104.jpg: 480x640 1 garchomp, Done. (1.548s)\n",
      "image 110/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_110.jpg: 512x640 1 garchomp, Done. (1.618s)\n",
      "image 111/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_115.png: 384x640 1 garchomp, Done. (1.230s)\n",
      "image 112/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_116.jpg: 640x480 1 garchomp, Done. (1.563s)\n",
      "image 113/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_122.png: 640x640 1 lucario, Done. (1.937s)\n",
      "image 114/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_129.jpg: 640x480 1 lucario, Done. (1.494s)\n",
      "image 115/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_134.jpg: 384x640 1 lucario, Done. (1.155s)\n",
      "image 116/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_135.jpg: 384x640 1 lucario, Done. (1.178s)\n",
      "image 117/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_140.jpg: 384x640 1 lucario, Done. (1.203s)\n",
      "image 118/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_141.jpg: 384x640 1 lucario, Done. (1.176s)\n",
      "image 119/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_146.jpg: 352x640 1 lucario, Done. (1.118s)\n",
      "image 120/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_147.jpg: 384x640 1 greninja, 3 lucarios, Done. (1.162s)\n",
      "image 121/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_153.jpg: 384x640 1 lucario, 2 thunduruss, Done. (1.150s)\n",
      "image 122/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_158.jpg: 384x640 1 thundurus, Done. (1.175s)\n",
      "image 123/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_159.png: 640x480 1 lucario, Done. (1.395s)\n",
      "image 124/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_164.jpg: 384x640 1 thundurus, Done. (1.170s)\n",
      "image 125/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_165.jpg: 640x480 Done. (1.442s)\n",
      "image 126/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_19.png: 640x512 1 eevee', Done. (1.522s)\n",
      "image 127/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_20.jpg: 640x448 1 eevee', 1 garchomp, Done. (1.402s)\n",
      "image 128/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_21.jpg: 640x576 1 pikachu, Done. (1.816s)\n",
      "image 129/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_30.jpg: 384x640 1 pikachu, 1 eevee', Done. (1.328s)\n",
      "image 130/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_31.png: 448x640 1 eevee', Done. (1.340s)\n",
      "image 131/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_32.png: 608x640 1 eevee', Done. (1.817s)\n",
      "image 132/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_33.png: 384x640 2 eevee's, Done. (1.113s)\n",
      "image 133/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_38.png: 256x640 Done. (0.840s)\n",
      "image 134/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_4.png: 640x640 Done. (1.850s)\n",
      "image 135/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_40.png: 640x640 1 staraptor, Done. (1.851s)\n",
      "image 136/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_42.png: 640x640 1 staraptor, Done. (1.815s)\n",
      "image 137/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_43.jpg: 640x640 Done. (1.821s)\n",
      "image 138/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_5.png: 384x640 1 pikachu, Done. (1.227s)\n",
      "image 139/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_50.jpg: 384x640 1 staraptor, Done. (1.651s)\n",
      "image 140/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_51.png: 288x640 1 staraptor, Done. (1.032s)\n",
      "image 141/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_56.jpg: 640x640 Done. (1.941s)\n",
      "image 142/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_57.jpg: 256x640 2 staraptors, Done. (0.780s)\n",
      "image 143/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_6.jpg: 384x640 1 pikachu, Done. (1.163s)\n",
      "image 144/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_7.jpg: 640x640 1 pikachu, Done. (1.866s)\n",
      "image 145/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_75.jpg: 608x640 Done. (1.741s)\n",
      "image 146/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_76.png: 448x640 1 infernape, Done. (1.333s)\n",
      "image 147/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_77.png: 512x640 3 infernapes, 2 staraptors, Done. (1.536s)\n",
      "image 148/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_78.png: 640x640 1 infernape, Done. (1.866s)\n",
      "image 149/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_79.jpg: 416x640 Done. (1.255s)\n",
      "image 150/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_8.png: 640x480 1 pikachu, Done. (1.490s)\n",
      "image 151/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_81.png: 640x640 1 infernape, Done. (1.819s)\n",
      "image 152/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_82.jpg: 384x640 1 infernape, Done. (1.160s)\n",
      "image 153/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_9.jpg: 384x640 Done. (1.223s)\n",
      "image 154/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_95.jpg: 320x640 2 greninjas, Done. (1.020s)\n",
      "image 155/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_96.jpg: 320x640 4 greninjas, Done. (0.972s)\n",
      "image 156/156 C:\\Users\\elise\\Desktop\\yolov5\\new_test\\val_97.png: 384x640 1 greninja, Done. (1.192s)\n",
      "Speed: 1.2ms pre-process, 1476.7ms inference, 1.1ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\exp10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''for i in range(2,4):\n",
    "    yesLife = f\"pokemon_images/images/train/{i}.jpg\"'''\n",
    "\n",
    "!python detect.py --weights 14_04_16_50.pt --img 640 --conf 0.25 --source new_test\n",
    "#display.Image(filename='runs/detect/exp7/lucario_06_04.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\elise/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  v6.0-195-gfd55271 torch 1.10.1 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 444 layers, 86705005 parameters, 0 gradients, 205.7 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "models.common.AutoShape"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5x')\n",
    "#model = torch.load(\"best_pokemon.pt\")\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkAzDWJ7cWTr"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eq1SMWl6Sfn"
   },
   "source": [
    "# 2. Validate\n",
    "Validate a model's accuracy on [COCO](https://cocodataset.org/#home) val or test-dev datasets. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be ~1% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyTZYGgRjnMc"
   },
   "source": [
    "## COCO val\n",
    "Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48,
     "referenced_widgets": [
      "eb95db7cae194218b3fcefb439b6352f",
      "769ecde6f2e64bacb596ce972f8d3d2d",
      "384a001876054c93b0af45cd1e960bfe",
      "dded0aeae74440f7ba2ffa0beb8dd612",
      "5296d28be75740b2892ae421bbec3657",
      "9f09facb2a6c4a7096810d327c8b551c",
      "25621cff5d16448cb7260e839fd0f543",
      "0ce7164fc0c74bb9a2b5c7037375a727",
      "c4c4593c10904cb5b8a5724d60c7e181",
      "473371611126476c88d5d42ec7031ed6",
      "65efdfd0d26c46e79c8c5ff3b77126cc"
     ]
    },
    "id": "WQPtK1QYVaD_",
    "outputId": "bcf9a448-1f9b-4a41-ad49-12f181faf05a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb95db7cae194218b3fcefb439b6352f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/780M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download COCO val\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../datasets && rm tmp.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X58w8JLpMnjH",
    "outputId": "74f1dfa9-6b6d-4b36-f67e-bbae243869f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\n",
      "YOLOv5 ðŸš€ v6.0-48-g84a8099 torch 1.10.0+cu102 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5x.pt to yolov5x.pt...\n",
      "100% 166M/166M [00:03<00:00, 54.1MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 444 layers, 86705005 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 2636.64it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../datasets/coco/val2017.cache\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:12<00:00,  2.17it/s]\n",
      "                 all       5000      36335      0.729       0.63      0.683      0.496\n",
      "Speed: 0.1ms pre-process, 4.9ms inference, 1.9ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.46s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=5.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=90.39s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=14.54s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.507\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.345\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.559\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.652\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.630\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.732\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.829\n",
      "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run YOLOv5x on COCO val\n",
    "!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rc_KbFk0juX2"
   },
   "source": [
    "## COCO test\n",
    "Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (**20,000 images, no labels**). Results are saved to a `*.json` file which should be **zipped** and submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0AJnSeCIHyJ"
   },
   "outputs": [],
   "source": [
    "# Download COCO test-dev2017\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017labels.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../datasets && rm tmp.zip\n",
    "!f=\"test2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f -d ../datasets/coco/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29GJXAP_lPrt"
   },
   "outputs": [],
   "source": [
    "# Run YOLOv5x on COCO test\n",
    "!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half --task test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY2VXXXu74w5"
   },
   "source": [
    "# 3. Train\n",
    "\n",
    "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png\"/></a></p>\n",
    "Close the active learning loop by sampling images from your inference conditions with the `roboflow` pip package\n",
    "<br><br>\n",
    "\n",
    "Train a YOLOv5s model on the [COCO128](https://www.kaggle.com/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n",
    "\n",
    "- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n",
    "automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n",
    "- **[Datasets](https://github.com/ultralytics/yolov5/tree/master/data)** available for autodownload include: [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/ultralytics/yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/ultralytics/yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/ultralytics/yolov5/blob/master/data/SKU-110K.yaml).\n",
    "- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n",
    "<br><br>\n",
    "\n",
    "## Train on Custom Data with Roboflow ðŸŒŸ NEW\n",
    "\n",
    "[Roboflow](https://roboflow.com/?ref=ultralytics) enables you to easily **organize, label, and prepare** a high quality dataset with your own custom data. Roboflow also makes it easy to establish an active learning pipeline, collaborate with your team on dataset improvement, and integrate directly into your model building workflow with the `roboflow` pip package.\n",
    "\n",
    "- Custom Training Example: [https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/?ref=ultralytics)\n",
    "- Custom Training Notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb)\n",
    "<br>\n",
    "\n",
    "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"480\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a275ad4b4ac20cd2e21a_roboflow-annotate.gif\"/></a></p>Label images lightning fast (including with model-assisted labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOy5KI2ncnWd"
   },
   "outputs": [],
   "source": [
    "# Tensorboard  (optional)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fLAV42oNb7M"
   },
   "outputs": [],
   "source": [
    "# Weights & Biases  (optional)\n",
    "%pip install -q wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NcFxRcFdJ_O",
    "outputId": "8724d13d-6711-4a12-d96a-1c655e5c3549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mgithub: \u001b[0m YOLOv5 is out of date by 29 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5  runs (RECOMMENDED)\n",
      "module 'signal' has no attribute 'SIGALRM'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=best_pokemon.pt, cfg=, data=coco128.yaml, hyp=data\\hyps\\hyp.scratch.yaml, epochs=3, batch_size=1, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "From https://github.com/ultralytics/yolov5\n",
      "   0cf932b..4c40933  master             -> origin/master\n",
      "   4c9e629..2a20f8c  classifier         -> origin/classifier\n",
      "   b63f2e7..4aff954  tests/aws          -> origin/tests/aws\n",
      " * [new branch]      update/social      -> origin/update/social\n",
      " * [new branch]      updates/benchmarks -> origin/updates/benchmarks\n",
      "YOLOv5  v6.0-195-gfd55271 torch 1.10.1 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=6 with nc=8\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n",
      "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
      "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
      "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
      "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n",
      "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
      "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
      "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
      "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n",
      "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n",
      " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
      " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
      " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
      " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
      " 24      [17, 20, 23]  1     87477  models.yolo.Detect                      [8, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
      "Model Summary: 567 layers, 86264917 parameters, 86264917 gradients, 204.3 GFLOPs\n",
      "\n",
      "Transferred 739/745 items from best_pokemon.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 123 weight (no decay), 126 weight, 126 bias\n",
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'pokemon_images\\labels\\train' images and labels...:   0%|          | 0/107 [00:00<?, ?it/s]C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'pokemon_images\\labels\\train' images and labels...1 found, 0 missing, 0 empty, 0 corrupt:   1%|          | 1/107 [00:07<13:53,  7.86s/it]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'pokemon_images\\labels\\train' images and labels...6 found, 0 missing, 0 empty, 0 corrupt:   6%|â–Œ         | 6/107 [00:07<01:39,  1.02it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'pokemon_images\\labels\\train' images and labels...13 found, 0 missing, 0 empty, 0 corrupt:  12%|â–ˆâ–        | 13/107 [00:08<00:34,  2.72it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'pokemon_images\\labels\\train' images and labels...35 found, 0 missing, 0 empty, 0 corrupt:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 35/107 [00:08<00:07, 10.13it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'pokemon_images\\labels\\train' images and labels...64 found, 0 missing, 0 empty, 0 corrupt:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 64/107 [00:08<00:01, 23.09it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'pokemon_images\\labels\\train' images and labels...101 found, 0 missing, 0 empty, 0 corrupt:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 101/107 [00:08<00:00, 44.44it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'pokemon_images\\labels\\train' images and labels...107 found, 0 missing, 0 empty, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107/107 [00:08<00:00, 12.73it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Cache directory pokemon_images\\labels is not writeable: [WinError 183] Impossible de crÃ©er un fichier dÃ©jÃ  existant: 'pokemon_images\\\\labels\\\\train.cache.npy' -> 'pokemon_images\\\\labels\\\\train.cache'\n",
      "\n",
      "  0%|          | 0/107 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram):  17%|â–ˆâ–‹        | 18/107 [00:00<00:01, 63.94it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 79/107 [00:00<00:00, 203.84it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107/107 [00:00<00:00, 240.16it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'pokemon_images\\labels\\val' images and labels...:   0%|          | 0/49 [00:00<?, ?it/s]C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'pokemon_images\\labels\\val' images and labels...1 found, 0 missing, 0 empty, 0 corrupt:   2%|â–         | 1/49 [00:07<06:05,  7.62s/it]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'pokemon_images\\labels\\val' images and labels...20 found, 0 missing, 0 empty, 0 corrupt:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/49 [00:07<00:08,  3.62it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'pokemon_images\\labels\\val' images and labels...49 found, 0 missing, 0 empty, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:07<00:00,  6.28it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING: Cache directory pokemon_images\\labels is not writeable: [WinError 183] Impossible de crÃ©er un fichier dÃ©jÃ  existant: 'pokemon_images\\\\labels\\\\val.cache.npy' -> 'pokemon_images\\\\labels\\\\val.cache'\n",
      "\n",
      "  0%|          | 0/49 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 19/49 [00:00<00:00, 171.62it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37/49 [00:00<00:00, 126.70it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:00<00:00, 174.84it/s]\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\elise\\anaconda3\\envs\\yolotest\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.55 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\train\\exp32\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "\n",
      "  0%|          | 0/107 [00:00<?, ?it/s]\n",
      "       0/2     1.79G   0.07998   0.03542   0.04219         4       640:   0%|          | 0/107 [00:04<?, ?it/s]\n",
      "       0/2     1.79G   0.07998   0.03542   0.04219         4       640:   0%|          | 0/107 [00:05<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 638, in <module>\n",
      "    main(opt)\n",
      "  File \"train.py\", line 535, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"train.py\", line 356, in train\n",
      "    callbacks.run('on_train_batch_end', ni, model, imgs, targets, paths, plots, opt.sync_bn)\n",
      "  File \"c:\\Users\\elise\\Desktop\\yolov5\\utils\\callbacks.py\", line 77, in run\n",
      "    logger['callback'](*args, **kwargs)\n",
      "  File \"c:\\Users\\elise\\Desktop\\yolov5\\utils\\loggers\\__init__.py\", line 90, in on_train_batch_end\n",
      "    self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\jit\\_trace.py\", line 741, in trace\n",
      "    return trace_module(\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\jit\\_trace.py\", line 958, in trace_module\n",
      "    module._c._create_method_from_trace(\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1090, in _slow_forward\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"c:\\Users\\elise\\Desktop\\yolov5\\models\\yolo.py\", line 126, in forward\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
      "  File \"c:\\Users\\elise\\Desktop\\yolov5\\models\\yolo.py\", line 149, in _forward_once\n",
      "    x = m(x)  # run\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1090, in _slow_forward\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"c:\\Users\\elise\\Desktop\\yolov5\\models\\common.py\", line 139, in forward\n",
      "    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1090, in _slow_forward\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1090, in _slow_forward\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"c:\\Users\\elise\\Desktop\\yolov5\\models\\common.py\", line 105, in forward\n",
      "    return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1090, in _slow_forward\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"c:\\Users\\elise\\Desktop\\yolov5\\models\\common.py\", line 47, in forward\n",
      "    return self.act(self.bn(self.conv(x)))\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1090, in _slow_forward\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 384, in forward\n",
      "    return F.silu(input, inplace=self.inplace)\n",
      "  File \"C:\\Users\\elise\\anaconda3\\envs\\yolotest\\lib\\site-packages\\torch\\nn\\functional.py\", line 1898, in silu\n",
      "    return torch._C._nn.silu_(input)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 0 bytes free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!python train.py --img 640 --batch 1 --epochs 3 --data coco128.yaml --weights best_pokemon.pt --cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15glLzbQx5u0"
   },
   "source": [
    "# 4. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLI1JmHU7B0l"
   },
   "source": [
    "## Weights & Biases Logging ðŸŒŸ NEW\n",
    "\n",
    "[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
    "\n",
    "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n",
    "\n",
    "<p align=\"left\"><img width=\"900\" alt=\"Weights & Biases dashboard\" src=\"https://user-images.githubusercontent.com/26833433/135390767-c28b050f-8455-4004-adb0-3b730386e2b2.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WPvRbS5Swl6"
   },
   "source": [
    "## Local Logging\n",
    "\n",
    "All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and val jpgs to see mosaics, labels, predictions and augmentation effects. Note an Ultralytics **Mosaic Dataloader** is used for training (shown below), which combines 4 images into 1 mosaic during training.\n",
    "\n",
    "> <img src=\"https://user-images.githubusercontent.com/26833433/131255960-b536647f-7c61-4f60-bbc5-cb2544d71b2a.jpg\" width=\"700\">  \n",
    "`train_batch0.jpg` shows train batch 0 mosaics and labels\n",
    "\n",
    "> <img src=\"https://user-images.githubusercontent.com/26833433/131256748-603cafc7-55d1-4e58-ab26-83657761aed9.jpg\" width=\"700\">  \n",
    "`test_batch0_labels.jpg` shows val batch 0 labels\n",
    "\n",
    "> <img src=\"https://user-images.githubusercontent.com/26833433/131256752-3f25d7a5-7b0f-4bb3-ab78-46343c3800fe.jpg\" width=\"700\">  \n",
    "`test_batch0_pred.jpg` shows val batch 0 _predictions_\n",
    "\n",
    "Training results are automatically logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) as `results.csv`, which is plotted as `results.png` (below) after training completes. You can also plot any `results.csv` file manually:\n",
    "\n",
    "```python\n",
    "from utils.plots import plot_results \n",
    "plot_results('path/to/results.csv')  # plot 'results.csv' as 'results.png'\n",
    "```\n",
    "\n",
    "<img align=\"left\" width=\"800\" alt=\"COCO128 Training Results\" src=\"https://user-images.githubusercontent.com/26833433/126906780-8c5e2990-6116-4de6-b78a-367244a33ccf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zelyeqbyt3GD"
   },
   "source": [
    "# Environments\n",
    "\n",
    "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
    "\n",
    "- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
    "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n",
    "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n",
    "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qu7Iesl0p54"
   },
   "source": [
    "# Status\n",
    "\n",
    "![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n",
    "\n",
    "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEijrePND_2I"
   },
   "source": [
    "# Appendix\n",
    "\n",
    "Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcKoSIK2WSzj"
   },
   "outputs": [],
   "source": [
    "# Reproduce\n",
    "for x in 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':\n",
    "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --task speed  # speed\n",
    "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMusP4OAxFu6"
   },
   "outputs": [],
   "source": [
    "# PyTorch Hub\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Images\n",
    "dir = 'https://ultralytics.com/images/'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGH0ZjkGjejy"
   },
   "outputs": [],
   "source": [
    "# CI Checks\n",
    "%%shell\n",
    "export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n",
    "rm -rf runs  # remove runs/\n",
    "for m in yolov5n; do  # models\n",
    "  python train.py --img 64 --batch 32 --weights $m.pt --epochs 1 --device 0  # train pretrained\n",
    "  python train.py --img 64 --batch 32 --weights '' --cfg $m.yaml --epochs 1 --device 0  # train scratch\n",
    "  for d in 0 cpu; do  # devices\n",
    "    python val.py --weights $m.pt --device $d # val official\n",
    "    python val.py --weights runs/train/exp/weights/best.pt --device $d # val custom\n",
    "    python detect.py --weights $m.pt --device $d  # detect official\n",
    "    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n",
    "  done\n",
    "  python hubconf.py  # hub\n",
    "  python models/yolo.py --cfg $m.yaml  # build PyTorch model\n",
    "  python models/tf.py --weights $m.pt  # build TensorFlow model\n",
    "  python export.py --img 64 --batch 1 --weights $m.pt --include torchscript onnx  # export\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gogI-kwi3Tye"
   },
   "outputs": [],
   "source": [
    "# Profile\n",
    "from utils.torch_utils import profile\n",
    "\n",
    "m1 = lambda x: x * torch.sigmoid(x)\n",
    "m2 = torch.nn.SiLU()\n",
    "results = profile(input=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVRSOhEvUdb5"
   },
   "outputs": [],
   "source": [
    "# Evolve\n",
    "!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n",
    "!d=runs/train/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs://bucket  # upload results (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSgFCAcMbk1R"
   },
   "outputs": [],
   "source": [
    "# VOC\n",
    "for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n",
    "  !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTRwsvA9u7ln"
   },
   "outputs": [],
   "source": [
    "# TensorRT \n",
    "# https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-pip\n",
    "!pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # install\n",
    "!python export.py --weights yolov5s.pt --include engine --imgsz 640 640 --device 0  # export\n",
    "!python detect.py --weights yolov5s.engine --imgsz 640 640 --device 0  # inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YOLOv5 Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ce7164fc0c74bb9a2b5c7037375a727": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25621cff5d16448cb7260e839fd0f543": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "384a001876054c93b0af45cd1e960bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25621cff5d16448cb7260e839fd0f543",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9f09facb2a6c4a7096810d327c8b551c",
      "value": "100%"
     }
    },
    "473371611126476c88d5d42ec7031ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5296d28be75740b2892ae421bbec3657": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65efdfd0d26c46e79c8c5ff3b77126cc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_473371611126476c88d5d42ec7031ed6",
      "value": " 780M/780M [00:11&lt;00:00, 91.9MB/s]"
     }
    },
    "65efdfd0d26c46e79c8c5ff3b77126cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "769ecde6f2e64bacb596ce972f8d3d2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f09facb2a6c4a7096810d327c8b551c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4c4593c10904cb5b8a5724d60c7e181": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dded0aeae74440f7ba2ffa0beb8dd612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4c4593c10904cb5b8a5724d60c7e181",
      "max": 818322941,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ce7164fc0c74bb9a2b5c7037375a727",
      "value": 818322941
     }
    },
    "eb95db7cae194218b3fcefb439b6352f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_384a001876054c93b0af45cd1e960bfe",
       "IPY_MODEL_dded0aeae74440f7ba2ffa0beb8dd612",
       "IPY_MODEL_5296d28be75740b2892ae421bbec3657"
      ],
      "layout": "IPY_MODEL_769ecde6f2e64bacb596ce972f8d3d2d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
